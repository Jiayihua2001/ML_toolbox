
Name                      : "Transformer"
mode                      : "full" # ["full", "dec_cond_lm", "dec_lm"]
pre_mode                  : "full" # ["full", "dec_cond_lm", "dec_lm"]

###### Dataset -----------------------------------------------------------------
# root                      : "/global/cfs/cdirs/m3578/yiqun/hw4p2/data/hw4p2"                # TODO: Set the root path of your data
root                      : "./data/hw4p2"                # TODO: Set the root path of your data
unpaired_text_partition   : "text-for-LM"               # unpaired text for LM pre-training
train_partition           : "train-clean-100"           # train-clean-100
val_partition             : "dev-clean"                 # validation partition
test_partition            : "test-clean"                # test partition
NUM_WORKERS               : 4
subset                    : 1       # Load a subset of the data (for debugging, testing, etc)
token_type                : "char"     # [char, 1k, 10k], char, 10k > 1k
feat_type                 : 'fbank'    # ['fbank', 'mfcc'], mfcc, fbank
num_feats                 : 80         # fbanks:[20-80], mfcc:[12:20], 60, 80 > 20, 40
batch_size                : 32  # 32
norm                      : 'cepstral' # ['global_mvn', 'cepstral']

###### SpecAugment ---------------------------------------------------------------
specaug                   : True
specaug_conf:
  apply_freq_mask         : True
  freq_mask_width_range   : 4 
  num_freq_mask           : 4 
  apply_time_mask         : True
  time_mask_width_range   : 50 
  num_time_mask           : 8 

###### Network Specs -------------------------------------------------------------
d_model                   : 512 
d_ff                      : 1024 
initialization            : "uniform" 
std                       : 0.02

###### Embedding Specs -----------------------------------------------------------
time_stride               : 4 
feature_stride            : 2 
embed_dropout             : 0.2 

###### Encoder Specs -------------------------------------------------------------
enc_dropout               : 0.1 
enc_num_layers            : 4 
enc_num_heads             : 8 

###### Decoder Specs -------------------------------------------------------------
dec_dropout               : 0.1 
dec_num_layers            : 4 
dec_num_heads             : 8 

###### Base Parameters -----------------------------------------------------------
use_wandb                 : True
resume                    : False
use_ctc                   : True
ctc_weight                : 0.5 # [0-0.01-0.1-1], 0.5, 5, 2, 1, 10
max_gradient_norm         : "None" # [None, 2, 1]
optimizer                 : "AdamW" # Adam, AdamW, SGD, AdamW >> SGD, Adam
momentum                  : 0.9
nesterov                  : True
learning_rate             : 2E-4 
scheduler                 : "None" # ['ReduceLR', 'CosineAnnealing'], ReduceLR, CosineAnnealing
factor                    : 0.2
patience                  : 2
pretrain_epochs           : 0 # maybe not important, [20-30]
train_epochs              : 60 # 

# python train.py