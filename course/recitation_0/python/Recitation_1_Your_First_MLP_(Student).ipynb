{"cells":[{"cell_type":"markdown","metadata":{"id":"cERUD7x8KGDM"},"source":["# ***Recitation 1: Writing Your First Multilayer Perceptron (MLP)***\n","\n","In this recitation, we'll explore the Multilayer Perceptron (MLP), a type of artificial neural network. We'll cover the intuition behind it, delve into the mathematical foundations, and finally, look at practical applications.\n","\n","## ***Some Quick Reminders:***\n","- Quiz 1 just released\n","- HW1 had just released\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LDJ5ddgPNaSM"},"source":["#***Part I: Intuition & Motivation***\n","--------------------------------------\n","--------------------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"9SR9zK8uI6QL"},"source":["\n","Perhaps the main intuition dictating, not just Deep Learning, but Machine Learning in general is this core idea:\n","\n","\n","## Machine Learning (ML) is nothing more than a set of techniques for constructing ***models*** to ***approximate functions*** which we ***do not know***, and these ***models are themselves functions***.\n","\n","\n","\n","In the case of Deep Learning, the model of consideration is, of course, deep neural networks however, there are other models as well, which have historically been used in Machine Learning such as: Linear and Logistic Regression Models, Support Vector Machines (SVMs), Decision Trees, Random Forests, Naive Bayes Classifiers, etc. Deep Learning is simply concerned with using Deep Neural Networks as our model, due to the recent explosion in capability of handling complicated tasks which they have demonstrated over these other model types.\n","\n","\n","<!-- We will use Manim and ManimML to vizualise some of the more general aspects of ML. This is the same as used by 3Blue1Brown to create his animations for his YouTube Videos. This might take a while. -->\n"]},{"cell_type":"markdown","metadata":{"id":"o23WsMOOUTvs"},"source":["#***Part II: Theory & Mathematical Foundations***\n","--------------------------------------------------\n","--------------------------------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"LZKfkPFuv_FW"},"source":["## ***Section 1:*** *Multilayer Perceptron Architecture*\n","---\n","\n","Multilayer Perceptrons (MLPs) are a specific type of feedforward neural network architecture that consists of multiple layers of interconnected neurons. These networks are designed to capture complex relationships in data through a sequence of transformations. Let's delve into the mathematical details of the architecture of MLPs and feedforward networks.\n","\n","Consider an MLP with the following specifications:\n","- Input Layer: Neurons receive input features represented by a vector **x** ∈ ℝⁿ $\\mathbf{x} \\in \\mathrm{R}^{n}$, where *n* is the number of input features.\n","- Hidden Layers: The network contains *L* hidden layers, each comprising *mᵢ* neurons. The *i*-th hidden layer is associated with a weight matrix **Wᵢ** and bias vector **bᵢ**.\n","- Output Layer: The final layer produces the output *y* ∈ ℝᵏ, where *k* is the number of output classes.\n","\n","Let's break down the architecture and computations involved in an MLP:\n","\n","**1. Input Layer:**\n","The input layer receives the raw feature vector **x**. Each neuron in this layer corresponds to a feature, and the input is propagated to the first hidden layer.\n","\n","**2. Hidden Layers:**\n","Each hidden layer *i* performs two primary computations:\n","\n","- **Weighted Sum:** The weighted sum **zᵢ** for the *i*-th hidden layer is calculated by combining the input vector **aᵢ₋₁** of the previous layer with the weight matrix **Wᵢ** and bias vector **bᵢ** of the current layer:\n","\n","  $$\\mathbf{z}_i = \\mathbf{W}_i \\mathbf{a}_{i-1} + \\mathbf{b}_i$$\n","  \n","  Here, **Wᵢ** ∈ ℝᵐ₍ᵢ₋₁₎×ᵐᵢ is the weight matrix of the *i*-th hidden layer, and **bᵢ** ∈ ℝᵐᵢ is the bias vector.\n","\n","- **Activation Function:** The activation function *fᵢ(z)* is applied element-wise to the weighted sum **zᵢ**, producing the activation vector **aᵢ**:\n","\n","  $$\\mathbf{a}_i = f_i(\\mathbf{z}_i)$$\n","\n","**3. Output Layer:**\n","The final hidden layer's activations **aₖ₋₁** serve as input to the output layer. The output layer computes the final prediction *y* using the weight matrix **Wₖ** and bias vector **bₖ**:\n","\n","$$\\mathbf{y} = \\mathbf{W}_k \\mathbf{a}_{k-1} + \\mathbf{b}_k$$\n","\n","Here, **Wₖ** ∈ ℝᵐₖ₋₁×ᵏ is the weight matrix of the output layer, and **bₖ** ∈ ℝᵏ is the bias vector.\n","\n","The architecture of an MLP emphasizes the concept of forward propagation. By sequentially processing the input through each layer, the network learns to capture increasingly abstract and complex features in the data. The computations involve linear combinations of inputs, bias terms, activation functions, and weight matrices. This architecture forms the basis for the network's ability to transform input data and make predictions.\n"]},{"cell_type":"markdown","metadata":{"id":"hBtMFzmdwgae"},"source":["## ***Section 2:*** *Forward Propagation*\n","---\n","\n","Forward propagation is the process through which an input signal propagates through the layers of a Multilayer Perceptron (MLP), resulting in the network's output prediction. Let's delve into the mathematical intricacies of forward propagation in an MLP.\n","\n","Given an input vector $\\mathbf{x} \\in \\mathrm{R}^{n}$ at the input layer, where $n$ is the number of input features, the forward propagation process involves the following steps for each hidden layer $i$:\n","\n","### ***Weighted Sum:***\n","The weighted sum $\\mathbf{z}_{i}$ for the $i^{\\text{th}}$ hidden layer is calculated by combining the input vector $\\mathbf{x}$ with the weight matrix $\\mathbf{W}_i$ and adding the bias vector $\\mathbf{b}_i$:\n","\n","$$\\mathbf{z}_i = \\mathbf{W}_i \\mathbf{x} + \\mathbf{b}_i$$\n","\n","Here, $\\mathbf{W}_i \\in \\mathrm{R}^{m_{\\,i-1}\\times\\,m_{\\,i}}$ is the weight matrix of the $i^{\\text{th}}$ hidden layer, where $m_{\\,i-1}$ is the number of neurons in the previous layer, $m_{\\,i}$ is the number of neurons in the current layer, and $\\mathbf{b}_i \\in \\mathrm{R}^{\\,m_i}$ is the bias vector.\n","\n","### ***Activation Function:***\n","The weighted sum **zᵢ** $\\mathbf{z}_i$ is then passed through an activation function *fᵢ(z)* $f_i(\\mathbf{z})$ element-wise, producing the activation vector **aᵢ**:\n","\n","$$\\mathbf{a}_i = f_i(\\mathbf{z}_i)$$\n","\n","Here, *fᵢ(z)* represents the activation function of the *i*-th hidden layer.\n","\n","This process is repeated for each hidden layer, with the output of one layer becoming the input for the next layer. Finally, the output layer produces the network's prediction:\n","\n","### ***Output Calculation:***\n","The output *y* ∈ ℝᵏ of the network is computed by applying the weight matrix **Wₖ** and bias vector **bₖ** to the activation vector **aₖ₋₁** of the last hidden layer:\n","\n","$$\\mathbf{y} = \\mathbf{W}_k \\mathbf{a}_{k-1} + \\mathbf{b}_k$$\n","\n","Here, **Wₖ** ∈ ℝᵐₖ₋₁×ᵏ is the weight matrix of the output layer, where *mₖ₋₁* is the number of neurons in the last hidden layer, and **bₖ** ∈ ℝᵏ is the bias vector.\n","\n","The final output *y* represents the network's prediction for the given input **x**.\n","\n","Forward propagation allows the MLP to transform raw input data through a sequence of weighted sums, activation functions, and output calculations. This process enables the network to extract hierarchical features from the input data and generate predictions based on the learned parameters. The mathematical operations at each layer lay the foundation for the network's ability to capture complex patterns and relationships in the data."]},{"cell_type":"markdown","metadata":{"id":"51v2n6WvA-_P"},"source":["## ***Section 3:*** *Loss Functions*\n","---\n","\n","A loss function quantifies the discrepancy between the predicted output of a Multilayer Perceptron (MLP) and the actual target values. This measure of dissimilarity serves as the basis for training the network by adjusting its parameters to minimize the loss. Let's explore the mathematical aspects of loss functions in an MLP.\n","\n","Given an MLP with input vector **x** ∈ ℝⁿ and target vector **ŷ** ∈ ℝᵏ, where *n* is the number of input features and *k* is the number of output classes, the goal is to minimize the difference between the predicted output **ŷ** and the actual target output **y**.\n","\n","**Mean Squared Error (MSE):**\n","One commonly used loss function is the Mean Squared Error, which measures the average squared difference between each element of the predicted output **ŷ** and the corresponding element of the target output **y**:\n","\n","$$\\text{MSE}(\\mathbf{ŷ}, \\mathbf{y}) = \\frac{1}{k} \\sum_{i=1}^{k} (ŷ_i - y_i)^2$$\n","\n","Here, *k* represents the number of output classes, *ŷᵢ* is the *i*-th element of the predicted output vector **ŷ**, and *yᵢ* is the corresponding *i*-th element of the target output vector **y**.\n","\n","**Cross-Entropy Loss:**\n","Another widely used loss function for classification tasks is the Cross-Entropy Loss, which measures the dissimilarity between the predicted probability distribution *p(ŷ)* and the true distribution *p(y)* of the target classes:\n","\n","$$\\text{CE}(\\mathbf{ŷ}, \\mathbf{y}) = - \\sum_{i=1}^{k} y_i \\log(ŷ_i)$$\n","\n","Here, *ŷᵢ* is the predicted probability of the *i*-th class, and *yᵢ* is the true label of the *i*-th class. The summation runs over all classes.\n","\n","Minimizing the loss function encourages the MLP to adjust its weights and biases to make its predictions more accurate and aligned with the true target values. During the training process, optimization algorithms, such as gradient descent, iteratively update the network's parameters to reduce the loss value.\n","\n","The choice of a loss function depends on the nature of the problem at hand. For regression tasks, Mean Squared Error is often used, while for classification tasks, Cross-Entropy Loss is preferred. The mathematical formulation of these loss functions provides a way to quantify the discrepancy between predictions and targets, guiding the network's learning process."]},{"cell_type":"markdown","metadata":{"id":"aoc0TlP5ByWO"},"source":["## ***Section 4:*** *Backpropagation*\n","---\n","\n","Backpropagation is a critical algorithm for training Multilayer Perceptrons (MLPs), enabling the adjustment of the network's weights and biases based on the computed gradients of the loss function. This process facilitates the optimization of the network's parameters to minimize the loss. Let's delve into the mathematical intricacies of backpropagation in an MLP.\n","\n","Consider an MLP with input vector **x** ∈ ℝⁿ and target vector **ŷ** ∈ ℝᵏ, where *n* is the number of input features and *k* is the number of output classes. Backpropagation involves the following steps:\n","\n","**1. Forward Propagation:**\n","Perform forward propagation through the network to compute the predicted output **ŷ**. This involves passing the input vector **x** through each layer, calculating weighted sums, applying activation functions, and producing intermediate activations.\n","\n","**2. Loss Calculation:**\n","Compute the loss between the predicted output **ŷ** and the target output **y** using an appropriate loss function, such as Mean Squared Error or Cross-Entropy Loss.\n","\n","**3. Gradient Calculation (Output Layer):**\n","Calculate the gradient of the loss with respect to the activations of the output layer, denoted as $\\frac{\\partial L}{\\partial \\mathbf{a}_{k-1}}$, where *L* represents the loss function.\n","\n","**4. Backpropagation Proper:**\n","Iterate backward through the layers, computing gradients with respect to activations and parameters. For each hidden layer *i*, the process involves the following steps:\n","\n","- **Gradient Calculation (Hidden Layer):**\n","  Compute the gradient of the loss with respect to the activations of the *i*-th hidden layer, denoted as $\\frac{\\partial L}{\\partial \\mathbf{a}_{i}}$.\n","\n","- **Gradient Propagation (Activation Function):**\n","  Propagate the gradient through the activation function *fᵢ(z)* of the *i*-th hidden layer, yielding $\\frac{\\partial L}{\\partial \\mathbf{z}_{i}}$.\n","\n","- **Gradient Propagation (Weighted Sum):**\n","  Propagate the gradient through the weighted sum operation, yielding gradients $\\frac{\\partial L}{\\partial \\mathbf{W}_{i}}$ and $\\frac{\\partial L}{\\partial \\mathbf{b}_{i}}$ for the weight matrix **Wᵢ** and bias vector **bᵢ** of the *i*-th hidden layer.\n","\n","**5. Weight Update:**\n","Using the computed gradients, update the weights and biases of the network's layers according to an optimization algorithm, such as gradient descent or its variants.\n","\n","The mathematical foundation of backpropagation relies on the chain rule of calculus. The chain rule enables the decomposition of the gradient of the loss function with respect to any parameter into a series of smaller gradients. These gradients are then used to guide the adjustments of weights and biases, leading to the minimization of the loss function.\n","\n","Backpropagation is a key reason why MLPs can effectively learn from data. By iteratively adjusting network parameters to minimize the loss, the network learns to make better predictions and capture patterns in the input data. The careful calculation of gradients at each layer forms the basis for the optimization process, facilitating the convergence of the network towards better solutions."]},{"cell_type":"markdown","metadata":{"id":"wb00SWXRCgAW"},"source":["## ***Section 5:*** *Gradient Descent & Optimization*\n","---\n","\n","Gradient descent is a fundamental optimization algorithm used to adjust the weights and biases of Multilayer Perceptrons (MLPs) during the training process. By iteratively updating these parameters, the algorithm guides the network towards finding optimal values that minimize the loss function. Let's delve into the mathematical intricacies of gradient descent and optimization in the context of an MLP.\n","\n","Consider an MLP with input vector **x** ∈ ℝⁿ and target vector **ŷ** ∈ ℝᵏ, where *n* is the number of input features and *k* is the number of output classes. The goal is to find optimal weight matrices **W** and bias vectors **b** that minimize the loss function *L*.\n","\n","### ***Gradient Descent:***\n","Gradient descent involves iteratively updating the parameters **W** and **b** in the opposite direction of the gradient of the loss function with respect to these parameters. The update rule for the *i*-th parameter can be expressed as follows:\n","\n","$$\\theta_{i+1} = \\theta_{i} - \\alpha \\frac{\\partial L}{\\partial \\theta_{i}}$$\n","\n","Here, *θᵢ* represents a parameter (weight or bias), ∂*L*/∂*θᵢ* is the gradient of the loss with respect to the parameter *θᵢ*, and *α* is the learning rate, a positive scalar controlling the step size of the updates.\n","\n","For example, for the weight matrix **Wᵢ** of the *i*-th hidden layer, the update rule is:\n","\n","$$\\mathbf{W}_i = \\mathbf{W}_i - \\alpha \\frac{\\partial L}{\\partial \\mathbf{W}_i} $$\n","\n","Gradient descent gradually shifts the parameters in the direction that reduces the loss, allowing the network to approach the optimal configuration.\n","\n","### ***Optimization Algorithms:***\n","While basic gradient descent can work, various optimization algorithms enhance the efficiency of weight updates. One such algorithm is Stochastic Gradient Descent (SGD), which updates parameters using small subsets (mini-batches) of the training data. Another popular algorithm is Adam, which adapts the learning rates for individual parameters based on their historical gradients.\n","\n","Mathematically, optimization algorithms leverage the gradients and learning rates to determine the optimal adjustments for the parameters, ensuring convergence to a suitable minimum of the loss function.\n","\n","The process of gradient descent and optimization underpins the training of MLPs. By iteratively fine-tuning weights and biases based on gradients, the network learns to make better predictions on new data. The mathematics of gradient descent provides a systematic approach for updating parameters and minimizing the loss function, making it a cornerstone of deep learning optimization."]},{"cell_type":"markdown","metadata":{"id":"GOn3uc3FDNZW"},"source":["## ***Section 6:*** *The Training Algorithm*\n","---\n","\n","The training of a Multilayer Perceptron (MLP) involves iteratively fine-tuning the network's parameters to minimize the loss function. Through forward and backward propagation, optimization algorithms adjust weights and biases to improve the network's ability to make accurate predictions. Let's delve into the mathematical intricacies of the training process in an MLP.\n","\n","Consider an MLP with input vector **x** ∈ ℝⁿ and target vector **ŷ** ∈ ℝᵏ, where *n* is the number of input features and *k* is the number of output classes. The training process comprises the following steps:\n","\n","**1. Forward Propagation:**\n","Given an input vector **x**, perform forward propagation through the network to compute the predicted output **ŷ**. This involves passing the input through each layer, calculating weighted sums, applying activation functions, and producing intermediate activations.\n","\n","**2. Loss Calculation:**\n","Compute the loss between the predicted output **ŷ** and the actual target output **y** using an appropriate loss function, such as Mean Squared Error or Cross-Entropy Loss.\n","\n","**3. Backpropagation and Gradient Calculation:**\n","Initiate backpropagation by computing the gradients of the loss with respect to the network's parameters. Starting from the output layer and moving backward, calculate gradients for each layer's activations, weighted sums, weights, and biases.\n","\n","**4. Weight Update:**\n","Utilize an optimization algorithm, such as gradient descent or its variants, to adjust the network's weights and biases based on the computed gradients. The update equations for the *i*-th hidden layer's weight matrix **Wᵢ** and bias vector **bᵢ** are:\n","\n","$$\\mathbf{W}_i = \\mathbf{W}_i - \\alpha \\frac{\\partial L}{\\partial \\mathbf{W}_i}$$\n","$$\\mathbf{b}_i = \\mathbf{b}_i - \\alpha \\frac{\\partial L}{\\partial \\mathbf{b}_i}$$\n","\n","Here, *α* is the learning rate, controlling the step size of weight updates.\n","\n","**5. Iterative Refinement:**\n","Repeat steps 1 to 4 for multiple epochs, allowing the network to progressively adjust its parameters and reduce the loss. Each epoch involves feeding the entire training dataset through the network and updating parameters based on the gradients.\n","\n","The training process aims to find parameters that minimize the loss function, enabling the network to generalize well to new, unseen data. By iteratively refining its weights and biases using the gradients, the network learns to make better predictions and capture underlying patterns in the training data.\n","\n","Mathematically, training an MLP involves optimizing the network's parameters using calculus, linear algebra, and optimization techniques. The mathematical foundation of training ensures that the network's predictions become progressively more accurate and aligned with the true target values, enhancing its capability to solve complex tasks."]},{"cell_type":"markdown","metadata":{"id":"Md9IgzWiEE7m"},"source":["## ***Section 7:*** *Overfitting & Regularization*\n","---\n","\n","Overfitting occurs when a Multilayer Perceptron (MLP) learns to perform exceedingly well on the training data but poorly on new, unseen data. Regularization techniques are employed to mitigate overfitting by introducing constraints on the network's complexity. Let's delve into the mathematical intricacies of overfitting, regularization, and how they affect an MLP.\n","\n","Consider an MLP with input vector **x** ∈ ℝⁿ and target vector **ŷ** ∈ ℝᵏ, where *n* is the number of input features and *k* is the number of output classes. Overfitting arises when the network captures noise or idiosyncrasies present in the training data, leading to poor generalization to new data.\n","\n","**1. Overfitting:**\n","Overfitting occurs when a network becomes too complex, fitting the training data's intricacies and noise rather than learning meaningful patterns. This leads to a low training loss but a high validation or test loss, indicating poor generalization.\n","\n","**2. Regularization Techniques:**\n","\n","   - **L2 Regularization (Weight Decay):**\n","     L2 regularization adds a penalty term to the loss function proportional to the sum of the squared magnitudes of the weights:\n","\n","     \\text{Loss} = \\text{Original Loss} + \\frac{\\lambda}{2} \\sum_{i} \\| \\mathbf{W}_i \\|_2^2\n","\n","     Here, *λ* is the regularization strength and ∥*Wᵢ*∥₂² represents the L2 norm of the weight matrix **Wᵢ** of the *i*-th layer.\n","\n","   - **Dropout:**\n","     Dropout randomly deactivates a fraction of neurons during each training iteration. This prevents the network from relying too heavily on specific neurons and encourages the network to learn robust representations.\n","\n","**3. Bias-Variance Trade-off:**\n","Regularization techniques address the bias-variance trade-off. A model with high complexity (low regularization) may have low bias but high variance, leading to overfitting. On the other hand, a simpler model (high regularization) may have higher bias but lower variance, potentially improving generalization.\n","\n","**4. Cross-Validation:**\n","Cross-validation is a technique for evaluating the model's performance on different subsets of the data to estimate its generalization ability. It helps identify whether the model is overfitting or generalizing well.\n","\n","Mathematically, regularization techniques introduce additional terms to the loss function, constraining the network's parameters and preventing them from growing too large. These techniques encourage the network to focus on essential features and patterns while reducing sensitivity to noise. By controlling the network's complexity, regularization fosters a balanced trade-off between fitting the training data and generalizing to new data."]},{"cell_type":"markdown","metadata":{"id":"jgB3fSOEFEBr"},"source":["## ***Section 8:*** *Hyperparameters & Tuning*\n","---\n","\n","\n","Hyperparameters are crucial settings that govern the behavior and learning process of a Multilayer Perceptron (MLP). These parameters are set before training and influence the network's architecture, optimization, and regularization. Let's delve into the mathematical intricacies of hyperparameters in the context of an MLP.\n","\n","Consider an MLP with input vector **x** ∈ ℝⁿ and target vector **ŷ** ∈ ℝᵏ, where *n* is the number of input features and *k* is the number of output classes. Hyperparameters play a pivotal role in shaping the training and performance of the network.\n","\n","**1. Learning Rate (*α*):**\n","The learning rate *α* determines the step size of weight updates during optimization. A small learning rate may lead to slow convergence, while a large learning rate can cause oscillations or divergence.\n","\n","**2. Number of Hidden Layers (*L*):**\n","The choice of the number of hidden layers *L* and their sizes influences the network's capacity to learn complex patterns. Deeper networks can capture intricate relationships, but they may also lead to overfitting.\n","\n","**3. Number of Neurons per Hidden Layer (*mᵢ*):**\n","The number of neurons in each hidden layer impacts the network's expressive power. Larger values can help capture more intricate features, but they also increase the risk of overfitting.\n","\n","**4. Activation Functions (*fᵢ*):**\n","The choice of activation functions affects the network's ability to model non-linear relationships. Common activation functions include the sigmoid, ReLU, and tanh.\n","\n","**5. Regularization Strength (*λ*):**\n","For regularization techniques like L2 regularization, the hyperparameter *λ* controls the balance between fitting the training data and avoiding overfitting. Larger values of *λ* increase the regularization effect.\n","\n","**6. Batch Size:**\n","The batch size determines how many examples are processed together before weight updates. Larger batch sizes can lead to faster training, but they may also consume more memory.\n","\n","**7. Number of Epochs:**\n","The number of training epochs defines how many times the entire dataset is fed to the network during training. Too few epochs may lead to underfitting, while too many epochs can cause overfitting.\n","\n","**8. Optimizer:**\n","The choice of optimization algorithm, such as SGD, Adam, or RMSProp, influences the network's convergence rate and training efficiency.\n","\n","Hyperparameter tuning involves finding the optimal combination of these settings to achieve the best model performance. This process often requires experimentation, cross-validation, and careful consideration of the trade-offs between bias and variance.\n","\n","Mathematically, adjusting hyperparameters involves modifying the learning rate, regularization strength, and architectural choices. The selection of appropriate values for these parameters is essential for training an MLP that effectively captures patterns in data while avoiding overfitting or poor convergence."]},{"cell_type":"markdown","metadata":{"id":"z7ev5XIpUHZW"},"source":["#***Part III: Praxis & Applications***\n","---------------------------------------\n","---------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"5JYpGrle1CHG"},"source":["## ***Section 1:*** *Applying An MLP to a Classical Example*\n","---\n","\n","For the purposes of this Recitation, we will look at the [MNIST Handwritten Digit Classification Example](https://www.kaggle.com/competitions/digit-recognizer) as the application for our MLP. This is considered to be the \"Hello, World!\" program of Deep Learning."]},{"cell_type":"markdown","metadata":{"id":"mK653ohe1Upw"},"source":["## ***Section 2:*** *Library Installation & Importing*\n","---"]},{"cell_type":"markdown","metadata":{"id":"LmPQr-iD1XKU"},"source":["We will also install the `torchsummaryX` library for surmmarizing network structures written in PyTorch."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"n6vgqRx61IJx"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/dell/anaconda3/envs/pytorch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["# Import Deep-Learning-Specific Libraries\n","import torch\n","import torchvision\n","from torchsummary import summary\n","import numpy as np\n","\n","# Import SciKit Learn for Additional Machine Learning Utilities\n","import sklearn\n","import sklearn.metrics\n","\n","# Import MatPlotLib and TQDM for Visualizing Our Input Data and Training Loop Progress.\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting torchsummary\n","  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n","Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n","Installing collected packages: torchsummary\n","Successfully installed torchsummary-1.5.1\n"]}],"source":["!pip install torchsummary"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2KKVQdpTcBTd","outputId":"a4ccb961-575b-48a3-9a7f-f84342332ae2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Device:  cpu\n"]}],"source":["# Ensure We Are Running on a GPU.\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(\"Device: \", device)\n","\n","# This check seems unimportant but if you didn't set up cuda before hand you'll have problems later. (This is done for you with colab and GCP but is a consideration for local)"]},{"cell_type":"markdown","metadata":{"id":"hwwzO1Es1d9u"},"source":["## ***Section 3:*** *Datasets & DataLoaders*\n","---\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"owP5TFRb1IiH","outputId":"2540b2f7-dd6f-42c3-ea1a-fa2dbeb0b6da"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./train/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 41988627.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./train/MNIST/raw/train-images-idx3-ubyte.gz to ./train/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./train/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 28881/28881 [00:00<00:00, 2155938.10it/s]"]},{"name":"stdout","output_type":"stream","text":["Extracting ./train/MNIST/raw/train-labels-idx1-ubyte.gz to ./train/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./train/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 19950070.38it/s]"]},{"name":"stdout","output_type":"stream","text":["Extracting ./train/MNIST/raw/t10k-images-idx3-ubyte.gz to ./train/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./train/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4542/4542 [00:00<00:00, 3317173.74it/s]"]},{"name":"stdout","output_type":"stream","text":["Extracting ./train/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./train/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./test/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 37828186.31it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./test/MNIST/raw/train-images-idx3-ubyte.gz to ./test/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./test/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 28881/28881 [00:00<00:00, 2737529.80it/s]"]},{"name":"stdout","output_type":"stream","text":["Extracting ./test/MNIST/raw/train-labels-idx1-ubyte.gz to ./test/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./test/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 16720156.36it/s]"]},{"name":"stdout","output_type":"stream","text":["Extracting ./test/MNIST/raw/t10k-images-idx3-ubyte.gz to ./test/MNIST/raw\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./test/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4542/4542 [00:00<00:00, 3068706.31it/s]"]},{"name":"stdout","output_type":"stream","text":["Extracting ./test/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./test/MNIST/raw\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Loading training dataset\n","transform = torchvision.transforms.ToTensor()\n","train_dataset = torchvision.datasets.MNIST('./train', download=True, train=True, transform=transform) # Consider this spot\n","\n","# Splitting training dataset to random dataset\n","train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [0.8,0.2])\n","test_dataset = torchvision.datasets.MNIST('./test', download=True, train=False, transform=transform)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"dctNkvp33Mo4"},"outputs":[],"source":["# Create data loaders\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n","val_loader   = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n","test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"bHW0OGpG3Xrm"},"source":["## ***Section 4:*** *Data Visualization*\n","---"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"683uKLwM3aWG"},"outputs":[],"source":["# Get first training sample & label pair\n","i=0\n","for x, y in train_dataset:\n","  i+=1\n","  if i>=1:\n","    break"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N341oNw-3r7i","outputId":"f866bcb4-f22d-4874-f8ba-a3ac00d3a491"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of channels:  1\n","Image height:  28\n","Image width:  28\n"]}],"source":["NUM_CHANNELS, HEIGHT, WIDTH = x.shape\n","print(\"Number of channels: \", NUM_CHANNELS)\n","print(\"Image height: \", HEIGHT)\n","print(\"Image width: \", WIDTH)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"82Bb1uIuwy1S","outputId":"ff82009e-bdf7-4748-f14d-e78b2c18f1f8"},"outputs":[{"data":{"text/plain":["torch.Size([28, 28])"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["x.squeeze().shape"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":452},"id":"B0glsMO85tE8","outputId":"4a03d5c1-8f07-4a5c-cb9d-ba46ffdd8586"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["npimg = x.numpy()\n","plt.imshow(npimg.squeeze())\n","plt.title(f\"Digit {str(y)}\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"nT8veslpGyib"},"source":["## ***Section 5:*** *Models*\n","---"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5_NdYpomGzVb","outputId":"1dc4ecca-08c0-4d14-8777-5283128a1494"},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Linear-1                 [-1, 1024]         803,840\n","              ReLU-2                 [-1, 1024]               0\n","            Linear-3                   [-1, 10]          10,250\n","================================================================\n","Total params: 814,090\n","Trainable params: 814,090\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.02\n","Params size (MB): 3.11\n","Estimated Total Size (MB): 3.12\n","----------------------------------------------------------------\n"]}],"source":["# Define some hyperparameters\n","IN_SIZE = HEIGHT * WIDTH\n","HIDDEN_SIZE = 1024\n","OUT_SIZE = 10\n","\"\"\"\n","Fill in the MLP below. You first need to create the initial linear layer that\n","takes size IN_SIZE and then outputs HIDDEN_SIZE. After this, you will need to have an\n","activation function and then the final linear layer which has the output size of OUT_SIZE.\n","\n","Below are a few links to pytorch documentation which you may find relevant.\n","The strongly recommended section is the minimum for the network where the additional\n","pages section contains documentation you may find helpful going into HW1P2 but are not required for this MLP.\n","\n","STRONGLY RECOMMENDED:\n","\n","    Linear Layers:          https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear\n","    Activation Functions:   https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity (Consider Relu or adjacent activation functions. Experiment with them, you may find different activations perform better sometimes.)\n","\n","POTENTIALLY RELAVENT PAGES:\n","    Dropout:                https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout\n","    Batchnorm:              https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d\n","\n","\"\"\"\n","\n","# Define your network (Feel free to create your networks)\n","model_1 = torch.nn.Sequential(\n","\n","  torch.nn.Linear(IN_SIZE,HIDDEN_SIZE), # Declare your first linear layer. What should the input and output size be?\n","\n","  torch.nn.ReLU(), # Declare your activation function\n","  # Add any additional layers here if you would like before the final layer\n","  torch.nn.Linear(HIDDEN_SIZE,OUT_SIZE) # Declare your final linear layer that outputs the OUT_SIZE\n",")\n","\n","# Move the model to device. This step is required before training\n","model_1 = model_1.to(device)\n","summary(model_1, x.flatten().shape)"]},{"cell_type":"markdown","metadata":{"id":"SUTr6gNT97dP"},"source":["## ***Section 6:*** *Training & Validation*\n","---"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"fHo3WVf2GzuN"},"outputs":[],"source":["\"\"\"\n","Fill in the training function below. To aid in testing the network above easier, a larger amount\n","is provided complete.\n","\"\"\"\n","\n","\n","def train_epoch(model, optimizer, criterion, data_loader):\n","    # Set the model to training mode\n","    model.train()\n","\n","    # Train loss counter for entire epoch\n","    train_loss = 0.0\n","\n","    for i, (images, labels) in enumerate(tqdm(data_loader)):\n","        # Move the data to device\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        # Run forward pass\n","        outputs = model(images.flatten(start_dim=1))\n","\n","        # Calculate loss using model outputs and labels\n","        loss = criterion(outputs,labels)\n","\n","        train_loss += loss # What are we adding to the training loss?\n","\n","        # Zero existing gradiant information from last step\n","        optimizer.zero_grad()\n","\n","        # Calculate gradient of the whole network using loss values\n","        loss.backward()\n","\n","        # Step the optimizer\n","        optimizer.step()\n","\n","    train_loss /= len(data_loader)\n","    return train_loss # What do we want to return?"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"qxTVNaBPHBYr"},"outputs":[],"source":["def eval(model, data_loader):\n","    model.eval()\n","\n","    true_labels = []\n","    pred_labels = []\n","\n","    for i, (images, labels) in enumerate(data_loader):\n","        images, labels = images.to(device), labels.to(device)\n","\n","        with torch.inference_mode():\n","            logits = model(images.flatten(start_dim=1))\n","\n","        preds = torch.argmax(logits, dim=1).squeeze()\n","        pred_labels.extend(preds.cpu().tolist())\n","        true_labels.extend(labels.cpu().tolist())\n","    accuracy = sklearn.metrics.accuracy_score(pred_labels, true_labels)\n","    return accuracy*100"]},{"cell_type":"markdown","metadata":{"id":"9ybQWmwbB3tA"},"source":["## ***Section 7:*** *Training*\n","---"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":500,"referenced_widgets":["74ef5097cda141f192dbb6b92974c08e","fa1841778b574653a9115f2c119b0494","39de01ad73d64374ba3ebd109b35e113","dec1c920e3534612aaf108155c6ff64d","b78e5ccd21474ac4b5ee5f76cce2c698","2826bb0f5f22410fac5ed463cdd4626f","9d753552f0474789a3d282577cc2d1b9","b1cd3e5f0cb54cb0b40533fd65130ee9","36fedbaf975e4c1b83cb38d8bfe0969e","d7ae2babadc74066a0e4d456c4dec72f","0c7ea4a0304a40eb92e93b24bcb3431a","4fc891ad4f9c4bc29688271cf4f058a2","8a2a6cacb1bb49299cac2d6e4fa14e72","5b182ddc41b449109f89ab41e8269fd5","dd7701ab5976437684ae7ba00bf33402","12270be8b5514a0f932335f1d454f28c","c6dd1686dd0548f2bb11b5cfa564c4b6","2eb252e9df6a40188d265c6b23feaa60","60aa21d233a04f4c93616e61d445bcb8","31c98944fd2342bc9e7529e73ca2b43f","95f4292a404941548f74ed276d616c7f","7fbf06c2f9724b5ab6ee2c10cf10480f","fd479e5ba548452d9dc25e9302a5a380","80ab046d8a6f465e98c5fa646eaa50bb","095339322615461997074f448f25c552","80bc7e98511949d5ab67e9b27c13b578","ed890f7677b844058a4186dc97a554d1","dfe2582c5d954445880210640210fe2e","3f85337ea4ac4d04a1f533bf1665948e","2bb7a12d50dc4b4b81e450d091387979","29d0b06fb7c048be9706e66d633fbf49","4e65ae42880847b6b03654f9e9b3f3a9","13968df237ee4acda03e7196559faca3","22b64b6fceb84af7a7fa7044add50705","23c4c8632b47476792ed7498206e41bc","95329572e4d84515ab1d4606b2806a1a","260935fb32744083b7338bf78c2aa6b8","478be37af81a4cda8a786881ca1c5175","2605020c32214ed59f330e5cb68f7002","98c7fc1db4194962b0d57cf7d29cca4b","641c9002f381487e8e39c6436bee101f","994d30a40f5b4d7f880c7e7dab5b14d8","133355201b9446e8b57aacb8004e58db","fe6996ac99344e1288e46aeabaff1665","4eff1ae08cd8452085597c5a2a0fedad","79661780776d46ce9869803b63476fef","787d7e9f5a0c4e9490a623842a45ff77","7511b415b7914d4dbeba2b22d8fc129d","be69b7e24e9e4c2ba5ea8005d27a9b4e","1c034e99d4d0497fa6909a7c42e08efa","c7ddd34440574e029afd03369e406bca","6f600e74c53e41fb8769777d08954f4e","4216b894ce9b4ef98a8df2142e8aa6a3","c6d99f44757a40bd83222a21b713b40f","d97b12a1299e463a89ce372812ee652f","413ea7701b3e4d88b3397114e6462acf","06c8c97d36e24a7fb7a372c1d8074918","8bf49c725ddb43719b247f3a412ee2f9","9b9d98edf9fc4437a3589b8ed29f301b","d2c5238d82194ef0b79aeec790ee318b","3c124728f90d4fae96d8a47d0b7c8e40","0cbf4bce0f5c49298eec61d7e6511a09","9f62426a73b74579b5446d1411f505a3","7e3705bb85e84a0caca2b38a83ebe95e","4abc95405c9043389f5c3223732f6f2e","25e0e73a61b44fc786b6bbc67556edf0","231398a117d649f3ae6a3ae83839fa56","a7c696460ffc47d68c69c968005fa095","4294483b94bb4971bc85b116e3b67422","a9542ad0df9a407fbaa8fc8072aad7a7","52755f9a279244f191be5daa8c481d59","78cf44db101f4f028aeac27d7018f9cd","7692271812834c16b607a561e403a93a","5a8a1479fb164b1a8a968585f68d3fb4","53234978279f4bf5b90fd0cb68579ff9","6a052956787341d88a1dae4df185e0e7","7aa00d4a53884b509b577634abdd7d8e","a7dda802c5ef4be89d2430f72011f80b","b9bbe9df484f4fc8be2d6be913af4b64","c2a75eecf3ed4be78da51715306eff2c","38f7eda9be0447f1b358aa1e298084e1","b589633e29454913ba20f13091afeca0","a01f556f57484e3cb4929b68c5679190","88347cac5c6f490cbf2e74d2355c488c","4a4928cdafdf49f78158646497d6ab71","99ea9c2fd0c741a3a868c58f25d89ab8","cf61bac31cd24c829a6174263e1ad671","dc47ca749421442a8151e89b408eb695","0ae3ff94c97d4896ade0f5891d0aff74","86fc1cb3847648feb9fa04f7d5c9435f","54b4bd46c3244b7da5b22021390ac58e","a65339546b46496ea28b5c14a1a337d3","426e868f66234687bb1ebe486e5c975a","3a955acda6324cc6a1a4bae7c3f370bb","70dede002ca04b439851fcef7936c6a0","115ae13d3bfa4056a579fc283ba42a89","566ee8f407a8486db1b290742c800fb3","b949ea95483949ffb3ef0dad80c2b7ae","f3082cf74b8845e39c0fe00488f09499","1a46daad0fe643a29e6302446b60dd54","0a0b56e0481c4ac0863959d68687777c","3a999cf31a39479285b27dbbfb3b77cf","aab7fc39df114545a7c7164fa07982b5","9eec0342d2a4421b8b316cd979bd99a6","b2ccc0d4cc4f482cad9af1a82a23c54f","5214b1d6a6524815bae94c6705204198","f8d4be93ef514d3ea98e1a1d955ca98c","2217febdff0345aa9fb87645cc864942","572fc3ac5e3d4eb08193670dd74a46ef","5e2dbf135d2b4cc3ae763aa698a21626"]},"id":"nbRpHK1EHQ2y","outputId":"f9624b04-7b21-434f-e73d-d5c48e2d0dbe"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 750/750 [00:07<00:00, 98.33it/s] \n"]},{"name":"stdout","output_type":"stream","text":["train_loss:  \tval_acc: 96.1\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 750/750 [00:07<00:00, 105.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["train_loss:  \tval_acc: 96.85000000000001\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 750/750 [00:07<00:00, 99.15it/s] \n"]},{"name":"stdout","output_type":"stream","text":["train_loss:  \tval_acc: 97.60833333333333\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 750/750 [00:07<00:00, 104.87it/s]\n"]},{"name":"stdout","output_type":"stream","text":["train_loss:  \tval_acc: 97.6\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 750/750 [00:06<00:00, 122.82it/s]\n"]},{"name":"stdout","output_type":"stream","text":["train_loss:  \tval_acc: 97.80833333333334\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 750/750 [00:09<00:00, 80.41it/s] \n"]},{"name":"stdout","output_type":"stream","text":["train_loss:  \tval_acc: 97.75833333333334\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 750/750 [00:05<00:00, 126.06it/s]\n"]},{"name":"stdout","output_type":"stream","text":["train_loss:  \tval_acc: 98.05\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 750/750 [00:07<00:00, 99.39it/s] \n"]},{"name":"stdout","output_type":"stream","text":["train_loss:  \tval_acc: 97.78333333333333\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 750/750 [00:07<00:00, 101.23it/s]\n"]},{"name":"stdout","output_type":"stream","text":["train_loss:  \tval_acc: 97.84166666666667\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 750/750 [00:08<00:00, 93.15it/s] \n"]},{"name":"stdout","output_type":"stream","text":["train_loss:  \tval_acc: 97.66666666666667\n"]}],"source":["optimizer   = torch.optim.Adam(model_1.parameters(), # How do we get model_1's parameters?\n","                                lr = 1e-3) # https://pytorch.org/docs/stable/optim.html\n","\n","                                # Get rid of adam and cross entropy loss\n","\n","criterion   = torch.nn.CrossEntropyLoss() # https://pytorch.org/docs/stable/nn.html#loss-functions\n","\n","NUM_EPOCHS = 10\n","\n","for epoch in range(NUM_EPOCHS):\n","    train_loss = train_epoch(model_1, optimizer, criterion, train_loader)\n","    val_acc    = eval(model_1, val_loader)\n","    print(f'train_loss: {train_loss} \\tval_acc: {val_acc}')"]},{"cell_type":"markdown","metadata":{"id":"aQC2bB_NtP_4"},"source":["# Bonus Section"]},{"cell_type":"markdown","metadata":{"id":"l1yKarinRzIp"},"source":["## Bonus: Label Smoothing"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"vL8uWG57R01x"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 750/750 [00:07<00:00, 103.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["train_loss:  \tval_acc: 93.675\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 750/750 [00:08<00:00, 84.30it/s] \n"]},{"name":"stdout","output_type":"stream","text":["train_loss:  \tval_acc: 93.34166666666667\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 750/750 [00:07<00:00, 95.13it/s] \n"]},{"name":"stdout","output_type":"stream","text":["train_loss:  \tval_acc: 93.76666666666667\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 750/750 [00:08<00:00, 90.36it/s] \n"]},{"name":"stdout","output_type":"stream","text":["train_loss:  \tval_acc: 94.04166666666667\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 750/750 [00:05<00:00, 142.72it/s]\n"]},{"name":"stdout","output_type":"stream","text":["train_loss:  \tval_acc: 94.53333333333333\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 750/750 [00:05<00:00, 145.27it/s]\n"]},{"name":"stdout","output_type":"stream","text":["train_loss:  \tval_acc: 93.89999999999999\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 750/750 [00:06<00:00, 122.63it/s]\n"]},{"name":"stdout","output_type":"stream","text":["train_loss:  \tval_acc: 93.70833333333334\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 750/750 [00:07<00:00, 101.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["train_loss:  \tval_acc: 93.03333333333333\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 750/750 [00:05<00:00, 136.98it/s]\n"]},{"name":"stdout","output_type":"stream","text":["train_loss:  \tval_acc: 93.79166666666666\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 750/750 [00:05<00:00, 133.63it/s]\n"]},{"name":"stdout","output_type":"stream","text":["train_loss:  \tval_acc: 93.49166666666666\n"]}],"source":["\n","# Please use your NN from above or create a new one. The same structure as above would work perfectly fine\n","\n","model_1 = torch.nn.Sequential(\n","                          torch.nn.Linear(IN_SIZE,HIDDEN_SIZE),\n","                          torch.nn.ReLU(),\n","                          torch.nn.Linear(HIDDEN_SIZE,OUT_SIZE)\n",")\n","model_1 = model_1.to(device)\n","\n","optimizer   = torch.optim.Adam(model_1.parameters(), lr= 0.03)\n","\n","# Read this documentation if you're not sure what to add for the label_smoothing parameter: https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html\n","criterion   = torch.nn.CrossEntropyLoss(label_smoothing=0.1) # What would be a good value for this? Consider that you don't want it to be quite too high\n","\n","NUM_EPOCHS = 10\n","\n","for epoch in range(NUM_EPOCHS):\n","    train_loss = train_epoch(model_1, optimizer, criterion, train_loader)\n","    val_acc    = eval(model_1, val_loader)\n","    print(f'train_loss: {train_loss} \\tval_acc: {val_acc}')"]},{"cell_type":"markdown","metadata":{"id":"w4HjmggCPTNo"},"source":["##Bonus: Improving your Networks and General Extra Reading\n"]},{"cell_type":"markdown","metadata":{"id":"4h4hPhZ6E3ld"},"source":["### Optimizers\n"]},{"cell_type":"markdown","metadata":{"id":"wBL-CfzJEBdl"},"source":["This section will discuss a few optimizers you may find useful going forward. Above we had used Adam, an extremely well explored optimizer which has many subsequent optimizers based on it. Among them for future homework you may find that the AdamW optimizer, or AdamP may be particularly interesting.\n","\n","\n","* AdamW's integrates weight decay, which makes it particularly suitable for complex models like CNNs, where it effectively helps in preventing overfitting, a common issue in such architectures. Weight decay works by gradually reducing the weights of the neurons in the network, encouraging simpler models and thus reducing overfitting.\n","\n","\n","* As for AdamP, it optimizes the model by adjusting the gradients in a way that is more sensitive to the structure of the data, potentially leading to more efficient and stable training.\n","\n","\n","* There is also the very classic approach of Stochastic Gradient Descent (SGD), which may prove to be a better optimizer in many cases, such as scenarios where simplicity and transparency in the optimization process are crucial. Unlike more complex optimizers, SGD relies on a straightforward update rule, which can be more interpretable and easier to debug. This makes it particularly effective in situations where the data is not excessively complex or when the model architecture is relatively simple. Moreover, SGD can sometimes escape local minima more effectively than algorithms like Adam, leading to better generalization in certain types of problems.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bP2qdimVGmMl"},"source":["Below will be a few simple code snippet examples of implementations of AdamW, AdamP, and SGD using PyTorch. Documentation will be provided, which I encourge you to read as well as the original papers. Going forward, think critically about why you would select a specific optimizer when creating a network as though the ones mentioned prior perform well generally, the small differences could help significantly in squeezing that last bit of performance out of a model."]},{"cell_type":"code","execution_count":34,"metadata":{"id":"VecucuHlEAaf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting adamp\n","  Downloading adamp-0.3.0.tar.gz (5.1 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hBuilding wheels for collected packages: adamp\n","  Building wheel for adamp (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for adamp: filename=adamp-0.3.0-py3-none-any.whl size=5981 sha256=4a3c393a5485bf1d199a00fa44383e9a2511776af23f0f680fef42418651c3c1\n","  Stored in directory: /Users/dell/Library/Caches/pip/wheels/33/f9/d6/b2ed816e1f321f6dcf72a99c954223b1259477095f40434979\n","Successfully built adamp\n","Installing collected packages: adamp\n","Successfully installed adamp-0.3.0\n"]},{"data":{"text/plain":["'\\nPAPERS:\\n\\nAdam:   https://arxiv.org/abs/1412.6980   \"Adam: A Method for Stochastic Optimization\"\\nAdamW:  https://arxiv.org/abs/1711.05101  \"AdamW: Decoupled Weight Decay Regularization\"\\nAdamP:  https://arxiv.org/abs/2006.08217  \"AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights\"\\n\\nSGD is well explored enough that this paper is included to discuss momentum. Read if you are interested, this topic may appear later.\\nSGD:    https://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\\n'"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["# Adam: The classic Adam Optimizer\n","optimizer   = torch.optim.Adam(model_1.parameters(), lr= 0.03) # https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam\n","\n","# AdamW: Notice when reading the pytorch documentation that AdamW allows a weight decay variable. This often improves performance by applying a form of regularization\n","optimizer   = torch.optim.AdamW(model_1.parameters(), lr= 0.03, weight_decay= 0.02) # https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.AdamW\n","\n","\n","!pip3 install adamp\n","\n","from adamp import AdamP\n","\n","# AdamP: AdamP is a bit more fiddly as it is not natively included in torch and you must import it like above, or write it yourself\n","\n","optimizer   = AdamP(model_1.parameters(), lr= 0.03) # Link: https://pypi.org/project/adamp/\n","\n","\n","# SGD: The classic SGD optimizer\n","\n","optimizer   = torch.optim.SGD(model_1.parameters(), lr= 0.03)  # Link: https://pytorch.org/docs/stable/generated/torch.optim.SGD.html\n","\n","\"\"\"\n","PAPERS:\n","\n","Adam:   https://arxiv.org/abs/1412.6980   \"Adam: A Method for Stochastic Optimization\"\n","AdamW:  https://arxiv.org/abs/1711.05101  \"AdamW: Decoupled Weight Decay Regularization\"\n","AdamP:  https://arxiv.org/abs/2006.08217  \"AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights\"\n","\n","SGD is well explored enough that this paper is included to discuss momentum. Read if you are interested, this topic may appear later.\n","SGD:    https://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"FtaUQCbdE53L"},"source":["### Dropout"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{"id":"0Pa0gsBaE802"},"source":["Dropout is a regularization technique used to prevent overfitting in neural networks. It works by randomly deactivating a subset of neurons during training, forcing the network to learn more robust features that are not reliant on any specific set of neurons, thus enhancing the model's generalization ability. Dropout is an extremely useful tool that you can find yourself using very often and will be very helpful in future assignments. You may even find it useful to tune dropout between epochs sometimes.\n"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"StBgVIKiE9bt"},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Linear-1                 [-1, 1024]         803,840\n","           Dropout-2                 [-1, 1024]               0\n","              ReLU-3                 [-1, 1024]               0\n","            Linear-4                   [-1, 10]          10,250\n","================================================================\n","Total params: 814,090\n","Trainable params: 814,090\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.02\n","Params size (MB): 3.11\n","Estimated Total Size (MB): 3.13\n","----------------------------------------------------------------\n"]}],"source":["\"\"\"\n","Please either create a network or fill in from earlier but with a dropout layer. Dropout is another tool that will show up\n","continually as you study deep learning. It is prolific and often takes other forms such as Drop Blocks and Drop Paths. The general concept for\n","what it does and the intuition is generally consistent however. Below implement the network using dropout. Documentation is provided below.\n","\n","Dropout:                https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout\n","\"\"\"\n","\n","# Define your network (Feel free to create your networks)\n","model_1 = torch.nn.Sequential(\n","\n","  torch.nn.Linear(IN_SIZE,HIDDEN_SIZE), # Declare your first linear layer. What should the input and output size be?\n","\n","  torch.nn.Dropout(), # Insert your dropout layer\n","\n","  torch.nn.ReLU(), # Declare your activation function\n","\n","\n","  # Add any additional layers here if you would like before the final layer\n","\n","\n","  torch.nn.Linear(HIDDEN_SIZE,OUT_SIZE) # Declare your final linear layer that outputs the OUT_SIZE\n",")\n","\n","# Move the model to device. This step is required before training\n","model_1 = model_1.to(device)\n","\n","summary(model_1, x.flatten().shape)"]},{"cell_type":"markdown","metadata":{"id":"dct4CIA2JY1T"},"source":["###Batch Normalization"]},{"cell_type":"markdown","metadata":{"id":"csnEHBVLJdQT"},"source":["Batch normalization (BatchNorm) is a technique used to improve the speed, performance, and stability of neural networks. It works by normalizing the inputs of each layer, ensuring that they have a mean of zero and a standard deviation of one, which helps to mitigate issues related to poor initialization and helps in faster convergence.\n","\n","BatchNorm though seeming to be quite a minor detail offers a significant impact on your network on more complex problems and can prove invaluable as an initial first step to improve your network. Below will be an example using code from before. Fill in with your neural network but this time, include batchnorm. This structure will be very helpful and repeat itself on future homeworks."]},{"cell_type":"code","execution_count":41,"metadata":{"id":"Xb3gF5jyJcqP"},"outputs":[],"source":["\"\"\"\n","Please either create a network or fill in from earlier but with the correct values for batchnorm. Note that\n","batchnorm needs to take in the size of the current prior layer to work. Usually this is done either\n","before an activation or after and for each layer. Documentation is provided below.\n","\n","Batchnorm:              https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d\n","\n","\"\"\"\n","\n","# Define your network (Feel free to create your networks)\n","model_1 = torch.nn.Sequential(\n","\n","  torch.nn.Linear(IN_SIZE,HIDDEN_SIZE), # Declare your first linear layer. What should the input and output size be?\n","\n","  torch.nn.BatchNorm2d(HIDDEN_SIZE), # Apply batch normalization\n","\n","  torch.nn.ReLU(), # Declare your activation function\n","\n","\n","  # Add any additional layers here if you would like before the final layer\n","\n","\n","  torch.nn.Linear(HIDDEN_SIZE,OUT_SIZE) # Declare your final linear layer that outputs the OUT_SIZE\n",")\n","\n","# Move the model to device. This step is required before training\n","model_1 = model_1.to(device)\n","\n","# summary(model_1, x.flatten(start_dim=0).to(device))"]},{"cell_type":"markdown","metadata":{"id":"AYlAWZfnxtyS"},"source":["### Schedulers"]},{"cell_type":"markdown","metadata":{"id":"fsjvQBe0xxSA"},"source":["Schedulers are tools used in training neural networks to adjust the learning rate during training, which can lead to more effective and efficient optimization. There are many viable options in this regard, including writing your own, but you may find ReduceLROnPlateau or CosineAnnealingLR especially useful.\n","\n","* ReduceLROnPlateau works by monitoring a specified metric (like validation loss) and reducing the learning rate when this metric stops improving. Essentially, it implements a form of learning rate decay; if the model's performance plateaus, the learning rate decreases, which can help to escape local minima and continue learning.\n","\n","* CosineAnnealingLR works by adjusting the learning rate following a cosine curve. This means the learning rate starts high, decreases to a minimum, and then increases again. This approach can help in navigating the loss landscape more effectively by allowing the model to explore more during the periods of higher learning rates and refine its understanding of the data during the periods of lower learning rates."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8oTFmtnxxyCS"},"outputs":[],"source":["# All optimizers built into Torch: https://pytorch.org/docs/stable/optim.html\n","\n","# ReduceLRonPlateau\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, ___) # https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau\n","\n","# CosineAnnealingLR\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, ___) # https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR\n","\n","\n","\"\"\"\n","PAPERS:\n","CosineAnnealingLR: https://arxiv.org/abs/1608.03983\n","\"\"\"\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"},"widgets":{"application/vnd.jupyter.widget-state+json":{"06c8c97d36e24a7fb7a372c1d8074918":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c124728f90d4fae96d8a47d0b7c8e40","placeholder":"​","style":"IPY_MODEL_0cbf4bce0f5c49298eec61d7e6511a09","value":"100%"}},"095339322615461997074f448f25c552":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2bb7a12d50dc4b4b81e450d091387979","max":750,"min":0,"orientation":"horizontal","style":"IPY_MODEL_29d0b06fb7c048be9706e66d633fbf49","value":750}},"0a0b56e0481c4ac0863959d68687777c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2ccc0d4cc4f482cad9af1a82a23c54f","placeholder":"​","style":"IPY_MODEL_5214b1d6a6524815bae94c6705204198","value":" 37%"}},"0ae3ff94c97d4896ade0f5891d0aff74":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_86fc1cb3847648feb9fa04f7d5c9435f","IPY_MODEL_54b4bd46c3244b7da5b22021390ac58e","IPY_MODEL_a65339546b46496ea28b5c14a1a337d3"],"layout":"IPY_MODEL_426e868f66234687bb1ebe486e5c975a"}},"0c7ea4a0304a40eb92e93b24bcb3431a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0cbf4bce0f5c49298eec61d7e6511a09":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"115ae13d3bfa4056a579fc283ba42a89":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12270be8b5514a0f932335f1d454f28c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"133355201b9446e8b57aacb8004e58db":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13968df237ee4acda03e7196559faca3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1a46daad0fe643a29e6302446b60dd54":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0a0b56e0481c4ac0863959d68687777c","IPY_MODEL_3a999cf31a39479285b27dbbfb3b77cf","IPY_MODEL_aab7fc39df114545a7c7164fa07982b5"],"layout":"IPY_MODEL_9eec0342d2a4421b8b316cd979bd99a6"}},"1c034e99d4d0497fa6909a7c42e08efa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2217febdff0345aa9fb87645cc864942":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"22b64b6fceb84af7a7fa7044add50705":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_23c4c8632b47476792ed7498206e41bc","IPY_MODEL_95329572e4d84515ab1d4606b2806a1a","IPY_MODEL_260935fb32744083b7338bf78c2aa6b8"],"layout":"IPY_MODEL_478be37af81a4cda8a786881ca1c5175"}},"231398a117d649f3ae6a3ae83839fa56":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a7c696460ffc47d68c69c968005fa095","IPY_MODEL_4294483b94bb4971bc85b116e3b67422","IPY_MODEL_a9542ad0df9a407fbaa8fc8072aad7a7"],"layout":"IPY_MODEL_52755f9a279244f191be5daa8c481d59"}},"23c4c8632b47476792ed7498206e41bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2605020c32214ed59f330e5cb68f7002","placeholder":"​","style":"IPY_MODEL_98c7fc1db4194962b0d57cf7d29cca4b","value":"100%"}},"25e0e73a61b44fc786b6bbc67556edf0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2605020c32214ed59f330e5cb68f7002":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"260935fb32744083b7338bf78c2aa6b8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_133355201b9446e8b57aacb8004e58db","placeholder":"​","style":"IPY_MODEL_fe6996ac99344e1288e46aeabaff1665","value":" 750/750 [00:20&lt;00:00, 39.07it/s]"}},"2826bb0f5f22410fac5ed463cdd4626f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29d0b06fb7c048be9706e66d633fbf49":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2bb7a12d50dc4b4b81e450d091387979":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2eb252e9df6a40188d265c6b23feaa60":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"31c98944fd2342bc9e7529e73ca2b43f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"36fedbaf975e4c1b83cb38d8bfe0969e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"38f7eda9be0447f1b358aa1e298084e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf61bac31cd24c829a6174263e1ad671","placeholder":"​","style":"IPY_MODEL_dc47ca749421442a8151e89b408eb695","value":" 750/750 [00:20&lt;00:00, 30.99it/s]"}},"39de01ad73d64374ba3ebd109b35e113":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1cd3e5f0cb54cb0b40533fd65130ee9","max":750,"min":0,"orientation":"horizontal","style":"IPY_MODEL_36fedbaf975e4c1b83cb38d8bfe0969e","value":750}},"3a955acda6324cc6a1a4bae7c3f370bb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a999cf31a39479285b27dbbfb3b77cf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8d4be93ef514d3ea98e1a1d955ca98c","max":750,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2217febdff0345aa9fb87645cc864942","value":275}},"3c124728f90d4fae96d8a47d0b7c8e40":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f85337ea4ac4d04a1f533bf1665948e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"413ea7701b3e4d88b3397114e6462acf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_06c8c97d36e24a7fb7a372c1d8074918","IPY_MODEL_8bf49c725ddb43719b247f3a412ee2f9","IPY_MODEL_9b9d98edf9fc4437a3589b8ed29f301b"],"layout":"IPY_MODEL_d2c5238d82194ef0b79aeec790ee318b"}},"4216b894ce9b4ef98a8df2142e8aa6a3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"426e868f66234687bb1ebe486e5c975a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4294483b94bb4971bc85b116e3b67422":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a8a1479fb164b1a8a968585f68d3fb4","max":750,"min":0,"orientation":"horizontal","style":"IPY_MODEL_53234978279f4bf5b90fd0cb68579ff9","value":750}},"478be37af81a4cda8a786881ca1c5175":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a4928cdafdf49f78158646497d6ab71":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4abc95405c9043389f5c3223732f6f2e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e65ae42880847b6b03654f9e9b3f3a9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4eff1ae08cd8452085597c5a2a0fedad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_79661780776d46ce9869803b63476fef","IPY_MODEL_787d7e9f5a0c4e9490a623842a45ff77","IPY_MODEL_7511b415b7914d4dbeba2b22d8fc129d"],"layout":"IPY_MODEL_be69b7e24e9e4c2ba5ea8005d27a9b4e"}},"4fc891ad4f9c4bc29688271cf4f058a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8a2a6cacb1bb49299cac2d6e4fa14e72","IPY_MODEL_5b182ddc41b449109f89ab41e8269fd5","IPY_MODEL_dd7701ab5976437684ae7ba00bf33402"],"layout":"IPY_MODEL_12270be8b5514a0f932335f1d454f28c"}},"5214b1d6a6524815bae94c6705204198":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52755f9a279244f191be5daa8c481d59":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53234978279f4bf5b90fd0cb68579ff9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"54b4bd46c3244b7da5b22021390ac58e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_115ae13d3bfa4056a579fc283ba42a89","max":750,"min":0,"orientation":"horizontal","style":"IPY_MODEL_566ee8f407a8486db1b290742c800fb3","value":750}},"566ee8f407a8486db1b290742c800fb3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"572fc3ac5e3d4eb08193670dd74a46ef":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a8a1479fb164b1a8a968585f68d3fb4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b182ddc41b449109f89ab41e8269fd5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_60aa21d233a04f4c93616e61d445bcb8","max":750,"min":0,"orientation":"horizontal","style":"IPY_MODEL_31c98944fd2342bc9e7529e73ca2b43f","value":750}},"5e2dbf135d2b4cc3ae763aa698a21626":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"60aa21d233a04f4c93616e61d445bcb8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"641c9002f381487e8e39c6436bee101f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a052956787341d88a1dae4df185e0e7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f600e74c53e41fb8769777d08954f4e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70dede002ca04b439851fcef7936c6a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"74ef5097cda141f192dbb6b92974c08e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fa1841778b574653a9115f2c119b0494","IPY_MODEL_39de01ad73d64374ba3ebd109b35e113","IPY_MODEL_dec1c920e3534612aaf108155c6ff64d"],"layout":"IPY_MODEL_b78e5ccd21474ac4b5ee5f76cce2c698"}},"7511b415b7914d4dbeba2b22d8fc129d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6d99f44757a40bd83222a21b713b40f","placeholder":"​","style":"IPY_MODEL_d97b12a1299e463a89ce372812ee652f","value":" 750/750 [00:19&lt;00:00, 34.34it/s]"}},"7692271812834c16b607a561e403a93a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"787d7e9f5a0c4e9490a623842a45ff77":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f600e74c53e41fb8769777d08954f4e","max":750,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4216b894ce9b4ef98a8df2142e8aa6a3","value":750}},"78cf44db101f4f028aeac27d7018f9cd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79661780776d46ce9869803b63476fef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c034e99d4d0497fa6909a7c42e08efa","placeholder":"​","style":"IPY_MODEL_c7ddd34440574e029afd03369e406bca","value":"100%"}},"7aa00d4a53884b509b577634abdd7d8e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e3705bb85e84a0caca2b38a83ebe95e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7fbf06c2f9724b5ab6ee2c10cf10480f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"80ab046d8a6f465e98c5fa646eaa50bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfe2582c5d954445880210640210fe2e","placeholder":"​","style":"IPY_MODEL_3f85337ea4ac4d04a1f533bf1665948e","value":"100%"}},"80bc7e98511949d5ab67e9b27c13b578":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e65ae42880847b6b03654f9e9b3f3a9","placeholder":"​","style":"IPY_MODEL_13968df237ee4acda03e7196559faca3","value":" 750/750 [00:28&lt;00:00, 41.77it/s]"}},"86fc1cb3847648feb9fa04f7d5c9435f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a955acda6324cc6a1a4bae7c3f370bb","placeholder":"​","style":"IPY_MODEL_70dede002ca04b439851fcef7936c6a0","value":"100%"}},"88347cac5c6f490cbf2e74d2355c488c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8a2a6cacb1bb49299cac2d6e4fa14e72":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6dd1686dd0548f2bb11b5cfa564c4b6","placeholder":"​","style":"IPY_MODEL_2eb252e9df6a40188d265c6b23feaa60","value":"100%"}},"8bf49c725ddb43719b247f3a412ee2f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f62426a73b74579b5446d1411f505a3","max":750,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7e3705bb85e84a0caca2b38a83ebe95e","value":750}},"95329572e4d84515ab1d4606b2806a1a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_641c9002f381487e8e39c6436bee101f","max":750,"min":0,"orientation":"horizontal","style":"IPY_MODEL_994d30a40f5b4d7f880c7e7dab5b14d8","value":750}},"95f4292a404941548f74ed276d616c7f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98c7fc1db4194962b0d57cf7d29cca4b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"994d30a40f5b4d7f880c7e7dab5b14d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"99ea9c2fd0c741a3a868c58f25d89ab8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9b9d98edf9fc4437a3589b8ed29f301b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4abc95405c9043389f5c3223732f6f2e","placeholder":"​","style":"IPY_MODEL_25e0e73a61b44fc786b6bbc67556edf0","value":" 750/750 [00:19&lt;00:00, 40.37it/s]"}},"9d753552f0474789a3d282577cc2d1b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9eec0342d2a4421b8b316cd979bd99a6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f62426a73b74579b5446d1411f505a3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a01f556f57484e3cb4929b68c5679190":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a65339546b46496ea28b5c14a1a337d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b949ea95483949ffb3ef0dad80c2b7ae","placeholder":"​","style":"IPY_MODEL_f3082cf74b8845e39c0fe00488f09499","value":" 750/750 [00:19&lt;00:00, 40.41it/s]"}},"a7c696460ffc47d68c69c968005fa095":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_78cf44db101f4f028aeac27d7018f9cd","placeholder":"​","style":"IPY_MODEL_7692271812834c16b607a561e403a93a","value":"100%"}},"a7dda802c5ef4be89d2430f72011f80b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b9bbe9df484f4fc8be2d6be913af4b64","IPY_MODEL_c2a75eecf3ed4be78da51715306eff2c","IPY_MODEL_38f7eda9be0447f1b358aa1e298084e1"],"layout":"IPY_MODEL_b589633e29454913ba20f13091afeca0"}},"a9542ad0df9a407fbaa8fc8072aad7a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a052956787341d88a1dae4df185e0e7","placeholder":"​","style":"IPY_MODEL_7aa00d4a53884b509b577634abdd7d8e","value":" 750/750 [00:21&lt;00:00, 40.80it/s]"}},"aab7fc39df114545a7c7164fa07982b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_572fc3ac5e3d4eb08193670dd74a46ef","placeholder":"​","style":"IPY_MODEL_5e2dbf135d2b4cc3ae763aa698a21626","value":" 275/750 [00:07&lt;00:12, 38.36it/s]"}},"b1cd3e5f0cb54cb0b40533fd65130ee9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2ccc0d4cc4f482cad9af1a82a23c54f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b589633e29454913ba20f13091afeca0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b78e5ccd21474ac4b5ee5f76cce2c698":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b949ea95483949ffb3ef0dad80c2b7ae":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9bbe9df484f4fc8be2d6be913af4b64":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a01f556f57484e3cb4929b68c5679190","placeholder":"​","style":"IPY_MODEL_88347cac5c6f490cbf2e74d2355c488c","value":"100%"}},"be69b7e24e9e4c2ba5ea8005d27a9b4e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2a75eecf3ed4be78da51715306eff2c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a4928cdafdf49f78158646497d6ab71","max":750,"min":0,"orientation":"horizontal","style":"IPY_MODEL_99ea9c2fd0c741a3a868c58f25d89ab8","value":750}},"c6d99f44757a40bd83222a21b713b40f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6dd1686dd0548f2bb11b5cfa564c4b6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7ddd34440574e029afd03369e406bca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cf61bac31cd24c829a6174263e1ad671":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2c5238d82194ef0b79aeec790ee318b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7ae2babadc74066a0e4d456c4dec72f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d97b12a1299e463a89ce372812ee652f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc47ca749421442a8151e89b408eb695":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dd7701ab5976437684ae7ba00bf33402":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95f4292a404941548f74ed276d616c7f","placeholder":"​","style":"IPY_MODEL_7fbf06c2f9724b5ab6ee2c10cf10480f","value":" 750/750 [00:24&lt;00:00, 18.24it/s]"}},"dec1c920e3534612aaf108155c6ff64d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7ae2babadc74066a0e4d456c4dec72f","placeholder":"​","style":"IPY_MODEL_0c7ea4a0304a40eb92e93b24bcb3431a","value":" 750/750 [00:18&lt;00:00, 43.06it/s]"}},"dfe2582c5d954445880210640210fe2e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed890f7677b844058a4186dc97a554d1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3082cf74b8845e39c0fe00488f09499":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f8d4be93ef514d3ea98e1a1d955ca98c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa1841778b574653a9115f2c119b0494":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2826bb0f5f22410fac5ed463cdd4626f","placeholder":"​","style":"IPY_MODEL_9d753552f0474789a3d282577cc2d1b9","value":"100%"}},"fd479e5ba548452d9dc25e9302a5a380":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_80ab046d8a6f465e98c5fa646eaa50bb","IPY_MODEL_095339322615461997074f448f25c552","IPY_MODEL_80bc7e98511949d5ab67e9b27c13b578"],"layout":"IPY_MODEL_ed890f7677b844058a4186dc97a554d1"}},"fe6996ac99344e1288e46aeabaff1665":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
