{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch offers a wide range of tensor operations that are essential for working with neural networks, data manipulation, and general-purpose computation. These operations are highly optimized and can run on both CPUs and GPUs. Below, Iâ€™ll outline some of the most commonly used PyTorch operations, categorized by their functionality.\n",
    "\n",
    "### 1. **Basic Tensor Operations**\n",
    "\n",
    "- **Creation**:\n",
    "  - `torch.tensor(data)`: Creates a tensor from data.\n",
    "  - `torch.zeros(size)`: Creates a tensor filled with zeros.\n",
    "  - `torch.ones(size)`: Creates a tensor filled with ones.\n",
    "  - `torch.eye(n)`: Creates an identity matrix.\n",
    "  - `torch.arange(start, end, step)`: Creates a 1D tensor with a range of values.\n",
    "  - `torch.linspace(start, end, steps)`: Creates a 1D tensor with linearly spaced values.\n",
    "\n",
    "- **Basic Arithmetic**:\n",
    "  - `torch.add(a, b)`: Element-wise addition.\n",
    "  - `torch.sub(a, b)`: Element-wise subtraction.\n",
    "  - `torch.mul(a, b)`: Element-wise multiplication.\n",
    "  - `torch.div(a, b)`: Element-wise division.\n",
    "  - `torch.pow(a, exponent)`: Element-wise power.\n",
    "\n",
    "- **Reduction**:\n",
    "  - `torch.sum(tensor)`: Sums all elements.\n",
    "  - `torch.mean(tensor)`: Computes the mean of all elements.\n",
    "  - `torch.prod(tensor)`: Computes the product of all elements.\n",
    "  - `torch.max(tensor)`: Returns the maximum value.\n",
    "  - `torch.min(tensor)`: Returns the minimum value.\n",
    "  - `torch.argmin(tensor)`: Returns the index of the minimum value.\n",
    "  - `torch.argmax(tensor)`: Returns the index of the maximum value.\n",
    "\n",
    "### 2. **Advanced Tensor Operations**\n",
    "\n",
    "- **Matrix Multiplication**:\n",
    "  - `torch.mm(a, b)`: Matrix multiplication of 2D tensors.\n",
    "  - `torch.matmul(a, b)`: General matrix multiplication (supports broadcasting).\n",
    "  - `torch.bmm(a, b)`: Batch matrix multiplication for 3D tensors.\n",
    "\n",
    "- **Element-wise Operations**:\n",
    "  - `torch.abs(tensor)`: Element-wise absolute value.\n",
    "  - `torch.exp(tensor)`: Element-wise exponential.\n",
    "  - `torch.log(tensor)`: Element-wise natural logarithm.\n",
    "  - `torch.sqrt(tensor)`: Element-wise square root.\n",
    "  - `torch.sin(tensor)`, `torch.cos(tensor)`: Element-wise trigonometric functions.\n",
    "\n",
    "- **Comparison Operations**:\n",
    "  - `torch.eq(a, b)`: Element-wise equality.\n",
    "  - `torch.ne(a, b)`: Element-wise inequality.\n",
    "  - `torch.gt(a, b)`: Element-wise greater than.\n",
    "  - `torch.ge(a, b)`: Element-wise greater than or equal.\n",
    "  - `torch.lt(a, b)`: Element-wise less than.\n",
    "  - `torch.le(a, b)`: Element-wise less than or equal.\n",
    "\n",
    "### 3. **Indexing, Slicing, and Reshaping**\n",
    "\n",
    "- **Indexing**:\n",
    "  - `tensor[index]`: Access elements using indices (works similarly to NumPy).\n",
    "  - `tensor[:, 0]`: Access all rows of the first column.\n",
    "\n",
    "- **Slicing**:\n",
    "  - `tensor[start:stop:step]`: Slices the tensor similar to Python lists or NumPy arrays.\n",
    "\n",
    "- **Reshaping**:\n",
    "  - `torch.reshape(tensor, shape)`: Reshapes the tensor to a specified shape.\n",
    "  - `tensor.view(shape)`: Another way to reshape a tensor.\n",
    "  - `tensor.transpose(dim0, dim1)`: Transposes the specified dimensions.\n",
    "  - `tensor.permute(*dims)`: Permutes the dimensions according to the specified order.\n",
    "  - `tensor.unsqueeze(dim)`: Adds a dimension of size 1 at the specified position.\n",
    "  - `tensor.squeeze(dim)`: Removes a dimension of size 1 from the specified position.\n",
    "\n",
    "### 4. **Broadcasting**\n",
    "\n",
    "PyTorch supports broadcasting, allowing tensors of different shapes to be used together in arithmetic operations. Broadcasting follows specific rules to match the dimensions:\n",
    "\n",
    "```python\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([[1], [2], [3]])\n",
    "result = a + b  # Broadcasts `a` to match `b`\n",
    "```\n",
    "\n",
    "### 5. **Linear Algebra Operations**\n",
    "\n",
    "- **Dot Product**:\n",
    "  - `torch.dot(a, b)`: Dot product of two 1D tensors.\n",
    "  \n",
    "- **Matrix Determinant**:\n",
    "  - `torch.det(matrix)`: Computes the determinant of a square matrix.\n",
    "  \n",
    "- **Matrix Inversion**:\n",
    "  - `torch.inverse(matrix)`: Computes the inverse of a square matrix.\n",
    "  \n",
    "- **Eigenvalues and Eigenvectors**:\n",
    "  - `torch.eig(matrix, eigenvectors=True)`: Computes the eigenvalues and optionally the eigenvectors of a square matrix.\n",
    "  \n",
    "- **Singular Value Decomposition (SVD)**:\n",
    "  - `torch.svd(matrix)`: Computes the singular value decomposition of a matrix.\n",
    "\n",
    "### 6. **Random Number Generation**\n",
    "\n",
    "- **Random Tensors**:\n",
    "  - `torch.rand(size)`: Generates a tensor with values uniformly distributed between 0 and 1.\n",
    "  - `torch.randn(size)`: Generates a tensor with values drawn from a standard normal distribution.\n",
    "  - `torch.randint(low, high, size)`: Generates a tensor with random integers between `low` (inclusive) and `high` (exclusive).\n",
    "\n",
    "### 7. **GPU Operations**\n",
    "\n",
    "PyTorch tensors can be moved between CPU and GPU:\n",
    "\n",
    "- **Moving Tensors to GPU**:\n",
    "  ```python\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  tensor = tensor.to(device)\n",
    "  ```\n",
    "\n",
    "- **Checking the Device**:\n",
    "  - `tensor.device`: Returns the device on which the tensor is located.\n",
    "\n",
    "### 8. **Autograd and Differentiation**\n",
    "\n",
    "PyTorch supports automatic differentiation through `autograd`:\n",
    "\n",
    "- **Tracking Gradients**:\n",
    "  ```python\n",
    "  tensor = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "  ```\n",
    "\n",
    "- **Computing Gradients**:\n",
    "  ```python\n",
    "  tensor = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "  output = tensor.sum()\n",
    "  output.backward()  # Computes gradients\n",
    "  print(tensor.grad)  # Access gradients\n",
    "  ```\n",
    "\n",
    "### Summary\n",
    "\n",
    "PyTorch provides a comprehensive set of tensor operations that are crucial for neural network operations, data manipulation, and mathematical computations. Understanding these operations is key to effectively using PyTorch for machine learning and scientific computing. If you have specific tensor operations or use cases in mind, feel free to ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ase'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mase\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ase'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
